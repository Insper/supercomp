# Data Race, Atomics e Throughput em GPU

Quando come√ßamos a ver sobre paralelismo em CPU com **OpenMP**, aprendemos que certas opera√ß√µes compartilhadas entre threads ‚Äî como somas globais ou atualiza√ß√µes em vetores ‚Äî exigem cuidados.
Usar um `#pragma omp critical` ou um `#pragma omp atomic` garante corre√ß√£o, mas tamb√©m gera um gargalo, pois apenas uma thread pode acessar aquele trecho de c√≥digo por vez.
Em muitos casos, podemos substituir o uso de `critical` por **estrat√©gias mais inteligentes**, como redu√ß√µes (`reduction`) ou vetores locais por thread, justamente para **evitar o custo da sincroniza√ß√£o**.

Em CUDA o racioc√≠nio √© o mesmo, mas em uma escala muito maior.
Uma GPU n√£o executa 8 ou 16 threads, e sim **milhares** √†s vezes **dezenas de milhares** de forma simult√¢nea.
Isso significa que qualquer ponto de conten√ß√£o, como uma opera√ß√£o `atomicAdd()` sobre um mesmo endere√ßo de mem√≥ria, pode eliminar completamente o paralelismo e fazer com que o desempenho da GPU drasticamente.

Por isso, **√© importante evitar ao m√°ximo o uso de opera√ß√µes at√¥micas**


## Por que o atomic √© t√£o custoso na GPU

Uma opera√ß√£o at√¥mica √© uma forma de **bloquear temporariamente** um endere√ßo de mem√≥ria enquanto uma thread o atualiza, impedindo que outra thread interfira.
Em CPU, o impacto √© pequeno, porque h√° poucas threads competindo.
Mas na GPU, a atomic vira um verdadeiro funil: centenas ou milhares de threads tentam acessar o mesmo dado ao mesmo tempo, e o hardware √© obrigado a **serializar os acessos**, uma thread por vez.

Em OpenMP, se voc√™ usar `#pragma omp atomic`, oito threads se alternam para atualizar uma vari√°vel o atraso √© percept√≠vel, mas suport√°vel.
Em CUDA, `atomicAdd()` pode ser disputado por **10.000 threads ao mesmo tempo**, e o tempo de espera se torna centenas de vezes maior.
Na pr√°tica, o throughput (quantidade de opera√ß√µes conclu√≠das por segundo) despenca.
O programa perde completamente o sentido de ser paralelo.


## Uma solu√ß√£o: reduzir a competi√ß√£o por mem√≥ria

A melhor forma de evitar atomics n√£o √© torcer para que elas fiquem baratas, mas **reorganizar o algoritmo** para que cada thread ou bloco trabalhe **em regi√µes de mem√≥ria diferentes**.
No exemplo do histograma, em vez de todas as threads atualizarem o mesmo vetor global, cada bloco de threads constr√≥i **seu pr√≥prio histograma local**, em *shared memory* (mem√≥ria compartilhada do bloco).
Essa mem√≥ria √© muito mais r√°pida, e como √© exclusiva daquele bloco, **n√£o h√° conflito entre blocos** portanto, **nenhum atomic √© necess√°rio**.

Cada thread do bloco incrementa contadores no seu histograma local de forma direta (sem precisar de `atomicAdd()` global), e no final o bloco escreve o resultado em uma regi√£o separada da mem√≥ria global.
Depois, uma etapa de fus√£o combina os histogramas locais para gerar o resultado final.
Essa fus√£o pode ser feita no host (CPU) ou em um segundo kernel da GPU.
Mesmo que use algumas atomics na fus√£o, o custo √© muito menor, porque agora temos poucos blocos competindo, e n√£o milhares de threads.


## Exemplo: tr√™s abordagens de histograma

Neste exemplo temos tr√™s implementa√ß√µes do mesmo histograma em CUDA:

1. **Vers√£o ing√™nua:**
   As threads incrementam diretamente o vetor `hist[bin]++`.
   √â a mais r√°pida, mas incorreta pois acontece *data race*.

2. **Vers√£o com `atomicAdd`:**
   Corrige o problema, mas for√ßa o hardware a serializar as opera√ß√µes.
   Funciona, mas √© como colocar `#pragma omp critical` dentro do loop cada incremento √© seguro, por√©m caro.

3. **Vers√£o com mem√≥ria compartilhada:**
   Cada bloco calcula seu histograma local em *shared memory* e depois os resultados s√£o somados.
   Essa abordagem evita o conflito global e preserva o paralelismo.
   √â o equivalente GPU da t√©cnica de *reduction* do OpenMP ‚Äî divide o trabalho, reduz localmente, combina no final.

```cpp
#include <iostream>
#include <vector>
#include <cuda_runtime.h>

//
// =====================================================
// Kernel ing√™nuo ‚Äî demonstra condi√ß√£o de corrida (data race)
// =====================================================
//
// Cada thread l√™ um valor do vetor `dados` e incrementa o contador
// do "chunk" correspondente no vetor global `histograma`.
//
// Problema: v√°rias threads podem tentar incrementar o mesmo √≠ndice
// ao mesmo tempo. Como a opera√ß√£o (ler ‚Üí somar ‚Üí escrever) n√£o √© at√¥mica,
// o resultado final se corrompe.
//
__global__ void histograma_ingenuo(const int *dados, int *histograma, int N, int numChunks) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        int chunk = dados[i];
        histograma[chunk]++;  // condi√ß√£o de corrida (data race)
    }
}

//
// =====================================================
// Kernel com atomicAdd ‚Äî correto, mas reduz throughput
// =====================================================
//
// A fun√ß√£o `atomicAdd()` garante exclusividade de acesso a um endere√ßo.
// Assim, o incremento √© seguro, mas o paralelismo efetivo diminui,
// pois v√°rias threads competem para acessar o mesmo chunk.
//
__global__ void histograma_atomico(const int *dados, int *histograma, int N, int numChunks) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        int chunk = dados[i];
        atomicAdd(&histograma[chunk], 1);  // funciona, por√©m destroi o paralelismo
    }
}

//
// =====================================================
// Kernel otimizado ‚Äî histograma local em mem√≥ria compartilhada
// =====================================================
//
// Cada bloco cria um histograma local na mem√≥ria compartilhada (`shared memory`),
// que √© muito mais r√°pida e exclusiva de cada bloco.
// Assim, evitamos o uso de opera√ß√µes at√¥micas globais.
// 
// Ap√≥s o c√°lculo local, cada bloco copia seu histograma parcial
// para a mem√≥ria global, e a fus√£o final √© feita na CPU.
//
__global__ void histograma_compartilhado(const int *dados, int *histogramas_blocos, int N, int numChunks) {
    extern __shared__ int hist_local[];  // mem√≥ria compartilhada din√¢mica
    int tid_global = blockIdx.x * blockDim.x + threadIdx.x;

    // --- Etapa 1: Inicializa o histograma local com zeros ---
    for (int i = threadIdx.x; i < numChunks; i += blockDim.x)
        hist_local[i] = 0;
    __syncthreads();

    // --- Etapa 2: Atualiza o histograma local ---
    if (tid_global < N) {
        int chunk = dados[tid_global];
        hist_local[chunk]++;
    }
    __syncthreads();

    // --- Etapa 3: Copia o histograma local para a mem√≥ria global ---
    for (int i = threadIdx.x; i < numChunks; i += blockDim.x)
        histogramas_blocos[blockIdx.x * numChunks + i] = hist_local[i];
}

//
// =====================================================
// Fus√£o dos histogramas locais na CPU
// =====================================================
//
// Ap√≥s cada bloco gerar seu histograma local na GPU,
// esta fun√ß√£o soma todos os histogramas parciais
// em um histograma final consolidado.
//
void fundir_histogramas_CPU(const std::vector<int> &histogramas_blocos,
                            std::vector<int> &histograma_final,
                            int numBlocos, int numChunks) {
    for (int b = 0; b < numBlocos; b++)
        for (int c = 0; c < numChunks; c++)
            histograma_final[c] += histogramas_blocos[b * numChunks + c];
}

//
// =====================================================
// Fun√ß√£o principal
// =====================================================
//
// Mede o tempo de execu√ß√£o de cada abordagem (ing√™nua, at√¥mica, compartilhada)
// e compara os resultados.
//
int main() {
    const int N = 1 << 20;        // 1 milh√£o de elementos
    const int numChunks = 256;    // quantidade de "caixas" do histograma
    const int tamBloco = 256;     // threads por bloco
    const int numBlocos = (N + tamBloco - 1) / tamBloco;

    std::cout << "=== HISTOGRAMA EM GPU ===\n";
    std::cout << "Elementos: " << N
              << " | Chunks: " << numChunks
              << " | " << numBlocos << " blocos x "
              << tamBloco << " threads\n\n";

    // -----------------------------
    // Aloca√ß√£o e inicializa√ß√£o no host
    // -----------------------------
    std::vector<int> h_dados(N);
    for (auto &v : h_dados) v = rand() % numChunks;

    std::vector<int> h_hist_ingenuo(numChunks, 0);
    std::vector<int> h_hist_atomico(numChunks, 0);
    std::vector<int> h_hist_compart(numChunks, 0);

    // -----------------------------
    // Aloca√ß√£o na GPU
    // -----------------------------
    int *d_dados = nullptr;
    int *d_hist = nullptr;
    int *d_hist_blocos = nullptr;
    cudaMalloc(&d_dados, N * sizeof(int));
    cudaMalloc(&d_hist, numChunks * sizeof(int));
    cudaMalloc(&d_hist_blocos, numBlocos * numChunks * sizeof(int));

    cudaMemcpy(d_dados, h_dados.data(), N * sizeof(int), cudaMemcpyHostToDevice);

    size_t tamMemCompart = numChunks * sizeof(int);

    // Vari√°veis para medir tempo
    cudaEvent_t inicio, fim;
    cudaEventCreate(&inicio);
    cudaEventCreate(&fim);
    float tempo_ingenuo = 0.0f, tempo_atomico = 0.0f, tempo_compart = 0.0f;

    // =====================================================
    // Vers√£o ing√™nua
    // =====================================================
    cudaMemset(d_hist, 0, numChunks * sizeof(int));
    cudaEventRecord(inicio);
    histograma_ingenuo<<<numBlocos, tamBloco>>>(d_dados, d_hist, N, numChunks);
    cudaEventRecord(fim);
    cudaEventSynchronize(fim);
    cudaEventElapsedTime(&tempo_ingenuo, inicio, fim);
    cudaMemcpy(h_hist_ingenuo.data(), d_hist, numChunks * sizeof(int), cudaMemcpyDeviceToHost);

    // =====================================================
    // Vers√£o at√¥mica
    // =====================================================
    cudaMemset(d_hist, 0, numChunks * sizeof(int));
    cudaEventRecord(inicio);
    histograma_atomico<<<numBlocos, tamBloco>>>(d_dados, d_hist, N, numChunks);
    cudaEventRecord(fim);
    cudaEventSynchronize(fim);
    cudaEventElapsedTime(&tempo_atomico, inicio, fim);
    cudaMemcpy(h_hist_atomico.data(), d_hist, numChunks * sizeof(int), cudaMemcpyDeviceToHost);

    // =====================================================
    // Vers√£o otimizada (mem√≥ria compartilhada)
    // =====================================================
    cudaEventRecord(inicio);
    histograma_compartilhado<<<numBlocos, tamBloco, tamMemCompart>>>(d_dados, d_hist_blocos, N, numChunks);
    cudaEventRecord(fim);
    cudaEventSynchronize(fim);
    cudaEventElapsedTime(&tempo_compart, inicio, fim);

    std::vector<int> h_hist_blocos(numBlocos * numChunks);
    cudaMemcpy(h_hist_blocos.data(), d_hist_blocos, numBlocos * numChunks * sizeof(int), cudaMemcpyDeviceToHost);
    fundir_histogramas_CPU(h_hist_blocos, h_hist_compart, numBlocos, numChunks);

    // =====================================================
    // C√°lculo do throughput (M ops/s)
    // =====================================================
    auto throughput = [&](float ms) {
        return static_cast<double>(N) / (ms / 1000.0) / 1e6;
    };

    double thr_ingenuo  = throughput(tempo_ingenuo);
    double thr_atomico  = throughput(tempo_atomico);
    double thr_compart  = throughput(tempo_compart);

    std::cout << "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n";
    std::cout << "Vers√£o         | Tempo (ms)      | Throughput (M ops/s)\n";
    std::cout << "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n";
    std::cout << "Ing√™nua        | " << tempo_ingenuo  << "         | " << thr_ingenuo  << "\n";
    std::cout << "At√¥mica        | " << tempo_atomico << "        | " << thr_atomico << "\n";
    std::cout << "Otimizada      | " << tempo_compart << "        | " << thr_compart << "\n";
    std::cout << "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n";

    // =====================================================
    // Libera√ß√£o de recursos
    // =====================================================
    cudaFree(d_dados);
    cudaFree(d_hist);
    cudaFree(d_hist_blocos);
    cudaEventDestroy(inicio);
    cudaEventDestroy(fim);

    return 0;
}

```

Lembre-se de carregar o modulo cuda dispon√≠vel, depois compile com: 

```
nvcc -Ofast hist.cu -o hist
```

Execute usando o srun:

```
srun --partition=gpu --gres=gpu:1 ./hist
```


Em programa√ß√£o paralela **sincronizar sempre tem custo**. Mas, na GPU, esse custo √© multiplicado por milhares de threads, e o impacto pode ser catastr√≥fico.
Por isso, **usar fun√ß√µes at√¥micas deve ser o √∫ltimo recurso**, reservado apenas para casos em que n√£o h√° outra forma de evitar uma condi√ß√£o de corrida.

A verdadeira otimiza√ß√£o em CUDA n√£o est√° em ‚Äúusar mais threads‚Äù, e sim em **organizar o trabalho de modo que cada thread e cada bloco acessem dados diferentes**.
Sempre que o acesso for independente, a GPU mostra toda sua for√ßa; quando h√° disputa, ela se comporta de forma lenta.

Perfeito üåü ‚Äî vamos montar uma explica√ß√£o **did√°tica e direta** de **throughput**, adaptada para o contexto desse c√≥digo CUDA (histograma com ‚Äúchunks‚Äù), com analogias que dialogam com o que seus alunos j√° viram em OpenMP e CPU paralela.

---

### O que √© **Throughput**

O termo **throughput** mede **a quantidade de trabalho que um sistema realiza por unidade de tempo**.
No nosso caso, ele indica **quantos incrementos (opera√ß√µes)** a GPU consegue fazer **por segundo**.

Em outras palavras:

> **Throughput = quantas opera√ß√µes o programa consegue realizar por segundo.**

Cada thread da GPU processa um elemento do vetor de entrada `dados[]` e incrementa o contador do ‚Äúchunk‚Äù correspondente no vetor `histograma[]`.
Logo, temos **N opera√ß√µes** (uma para cada elemento).

O tempo total de execu√ß√£o (`tempo_ms`) √© medido com os eventos do CUDA (`cudaEventRecord`).


## F√≥rmula utilizada no c√≥digo

O throughput √© calculado como:

[
\text{Throughput (M ops/s)} = \frac{N}{t_s} \div 10^6
]

onde:

| S√≠mbolo  | Significado                                                  |
| :------- | :----------------------------------------------------------- |
| ( N )    | N√∫mero total de opera√ß√µes executadas (elementos processados) |
| ( t_s )  | Tempo total do kernel em segundos                            |
| ( 10^6 ) | Convers√£o para ‚Äúmilh√µes de opera√ß√µes por segundo‚Äù            |

Como `cudaEventElapsedTime()` retorna o tempo em **milissegundos**, fazemos:

[
t_s = \frac{t_{ms}}{1000}
]

e portanto, no c√≥digo:

```cpp
auto throughput = [&](float ms) {
    return static_cast<double>(N) / (ms / 1000.0) / 1e6;
};
```

